{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit 5.1 -  Neural Networks Building Blocks\n",
    "\n",
    "Neural networks are made of smaller modules or building blocks, similarly to atoms in matter and logic gates in digital circuits.\n",
    "\n",
    "Once you know what the blocks are, you can combine them to solve a variety of problems. Many new building blocks are added constantly, as smart minds discover them\n",
    "\n",
    "What are these blocks? How can we use them? Follow me…\n",
    "\n",
    "## Weights\n",
    "\n",
    "Weights implement the  product of an input _i_ with a weight value _w,_ to produce an output _o_. Seems easy, but addition and multiplications are at the hearth of neural networks.\n",
    "\n",
    "![](images/nn-blocks-1.png)\n",
    "\n",
    "weights implement the product of an input i with a weight value w to give an output o\n",
    "\n",
    "## Neurons\n",
    "\n",
    "Artificial neurons are one basic building block.\n",
    "\n",
    "![](images/nn-blocks-2.png)\n",
    "\n",
    "neuron, inputs i, output o, weight w\n",
    "\n",
    "They sum up some inputs  _i_  and use a  non-linearity  (added as separate block) to output (_o_) a non-liner version of the sum.\n",
    "\n",
    "Neurons use weights _w_ to take inputs and scale them, before summing the values.\n",
    "\n",
    "## Identity layers\n",
    "\n",
    "Identity layers just pass the input to the output.  Seem pretty bare, but they are an essential block of neural networks.\n",
    "\n",
    "![](images/nn-blocks-3.png)\n",
    "\n",
    "identity: i=o\n",
    "\n",
    "See below why. You will not be disappointed!\n",
    "\n",
    "## Non-linearities\n",
    "\n",
    "[http://pytorch.org/docs/master/nn.html#non-linear-activations](https://pytorch.org/docs/master/nn.html#non-linear-activations)\n",
    "\n",
    "They take  the inner value of a neuron  and  transform  it with a  non-linerfunction  to produce a neuron output.\n",
    "\n",
    "![](images/nn-blocks-4.png)\n",
    "\n",
    "non-linear module\n",
    "\n",
    "The most popular are:  ReLU,  tang,  sigmoid.\n",
    "\n",
    "## Linear layers\n",
    "\n",
    "[http://pytorch.org/docs/master/nn.html#linear-layers](https://pytorch.org/docs/master/nn.html#linear-layers)\n",
    "\n",
    "These layers are an array of neurons. Each take multiple inputs and produce multiple outputs.\n",
    "\n",
    "![](images/nn-blocks-5.png)\n",
    "\n",
    "linear layer: input i and output o are vectors of any and different length. A weight matrix (left) and how it may be fully connected (right) show different version of the same linear layer.\n",
    "\n",
    "The input and output number is arbitrary and of uncorrelated length.\n",
    "\n",
    "These building blocks are basically an array of  linear  combinations of inputs scaled by weights.  Weights multiply inputs with an array of weight values, and they are usually learned with a learning algorithm.\n",
    "\n",
    "Linear layers do not come with non-linearities, you will have to add it after each layer.  If you stack multiple layers, you will need a non-linearity between them, or they will all collapse to a single linear layer.\n",
    "\n",
    "## Convolutional layers\n",
    "\n",
    "[http://pytorch.org/docs/master/nn.html#convolution-layers](https://pytorch.org/docs/master/nn.html#convolution-layers)\n",
    "\n",
    "They are exactly like a linear layer, but each output neuron is connected to a locally constrained group of input neurons.\n",
    "\n",
    "![](images/nn-blocks-6.png)\n",
    "\n",
    "convolution is a 3d matrix operation: an inputs cuboid of data is multiplied by multiple kernels to produce an output cuboid of data. These are just a lot of multiplication and additions\n",
    "\n",
    "This group is often called receptive-field, borrowing the name from neuroscience.\n",
    "\n",
    "Convolutions can be performed in 1D, 2D, 3D… etc.\n",
    "\n",
    "These basic block takes advantage of the local features of data, which are correlated in some kind of inputs.  If the inputs are correlated, then it makes more sense to look at a group of inputs rather than a single value like in a linear layer. Linear layer can be thought of a convolutions with 1 value per filters.\n",
    "\n",
    "## Recurrent neural network\n",
    "\n",
    "[http://pytorch.org/docs/master/nn.html#rnn](https://pytorch.org/docs/master/nn.html#recurrent-layers)\n",
    "\n",
    "Recurrent neural networks are a special kind of linear layer, where the output of each neuron is fed back ad additional input of the neuron,  together with actual input.\n",
    "\n",
    "![](images/nn-blocks-7.png)\n",
    "\n",
    "RNN: outputs or an additional state are fed back (values at time t-1) as an input together with input i (at time t)\n",
    "\n",
    "You can think of RNN as a combination of the input at instant _t_ and the state/output of the same neuron at time _t-1_.\n",
    "\n",
    "RNN can remember sequences of values, since they can recall previous outputs, which again are linear combinations of their inputs.\n",
    "\n",
    "## Attention modules\n",
    "\n",
    "Attention modules are simply a gating function for memories. If you want to specify which values in an array should be passed through attention, you use a linear layer to gate each input by some weighting function.\n",
    "\n",
    "![](images/nn-blocks-8.png)\n",
    "\n",
    "Attention module:  an input i vector is linearly combined by multiplying each value by a weight matrix and producing one or multiple outputs\n",
    "\n",
    "Attention modules can be soft when the weights are real-valued and the inputs are thus multiplied by values. Attention is hard when weight are binary, and inputs are either 0 or passing through.  Outputs are also called attention head outputs. More info:  [nice blog post](https://distill.pub/2016/augmented-rnns/)  and  [another](http://akosiorek.github.io/visual-attention/).\n",
    "\n",
    "## Memories\n",
    "\n",
    "A basic building block that saves multiple values  in a  table.\n",
    "\n",
    "![](images/nn-blocks-9.png)\n",
    "\n",
    "memory, input i, ouput o, optional addressing a\n",
    "\n",
    "This is basically just a multi-dimensional array. The table can be of any size and any dimensions. It can also be composed of multiple banks or heads.  Generally memory have a write function writes to all locations and reads from all location.  An attention-like module can focus reading and writing to specific locations.  See attention module below. More info:  [nice blog post](https://distill.pub/2016/augmented-rnns/).\n",
    "\n",
    "## Residual modules\n",
    "\n",
    "Residual modules are a simple combination of layers and pass-through layers or Identity layers (ah-ah! told you they were important!).\n",
    "\n",
    "![](images/nn-blocks-10.png)\n",
    "\n",
    "residual module can combine input with a cascade of other layers\n",
    "\n",
    "They became famous in ResNet: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385), and they have revolutionized neural networks since.\n",
    "\n",
    "## LSTM, GRU, etc.\n",
    "\n",
    "[http://pytorch.org/docs/master/nn.html#recurrent-layers](https://pytorch.org/docs/master/nn.html#recurrent-layers)\n",
    "\n",
    "These units use an RNN, residual modules and multiple Attention modules to gate inputs, outputs and state values (and more) of an RNN,  to produce augmented RNN that can remember longer sequence in the future. More info and figures:  [great post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  and  [nice blog post](https://distill.pub/2016/augmented-rnns/).\n",
    "\n",
    "## Neural networks\n",
    "\n",
    "Now the hard part: how do you combine these modules to make neural networks that can actually solve interesting problems in the world? See next lesson!\n",
    "\n",
    "So far it has been  human smart minds  that  architected these neural network topologies.\n",
    "\n",
    "But why, did you not tell me that neural network were all about learning from data? And yet the most important things or all, the network architecture is still designed by hand? What?\n",
    "\n",
    "### ... This list will grow, stay tuned ...\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "This lecture material comes from this [blog post](https://euge-blog.github.io/2017/11/01/nn-building-blocks.html) or thi [blog post](https://culurciello.medium.com/neural-networks-building-blocks-a5c47bcd7c8d), originally written in 2017.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
