{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# unit 1.3 - Back-propagation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/culurciello/deep-learning-course-source/blob/main/source/lectures/13-backpropagation.ipynb)\n",
    "\n",
    "Backpropagation is an optimization algorithm commonly used in training artificial neural networks. It's a supervised learning algorithm that adjusts the weights of the network to minimize the error between predicted and actual outputs.\n",
    "\n",
    "## An example\n",
    "\n",
    "Let's consider a simple two-layer neural network with two inputs (x1 and x2), two neurons in the first layer (hidden layer), and two neurons in the second layer (output layer). We'll use a mean squared error (MSE) loss function.\n",
    "\n",
    "Here's the architecture:\n",
    "- Inputs: x1, x2\n",
    "- Hidden layer neurons: h1, h2\n",
    "- Output layer neurons: o1, o2\n",
    "\n",
    "![](images/backpropagation.png)\n",
    "\n",
    "The forward pass can be expressed as follows:\n",
    "\n",
    "$h_1 = w_{11} \\cdot x_1 + w_{21} \\cdot x_2 + b_1$\n",
    "\n",
    "$h_2 = w_{12} \\cdot x_1 + w_{22} \\cdot x_2 + b_2$\n",
    "\n",
    "$o_1 = w_{31} \\cdot h_1 + w_{41} \\cdot h_2 + b_3$\n",
    "\n",
    "$o_2 = w_{32} \\cdot h_1 + w_{42} \\cdot h_2 + b_4$\n",
    "\n",
    "Here, $w$ represents weights, $b$ represents biases, and the subscripts denote the connection between neurons.\n",
    "\n",
    "Let's assume the target outputs are $y_1$ and $y_2$. The MSE loss is given by:\n",
    "$L = \\frac{1}{2} \\sum_{i=1}^{2} (y_i - o_i)^2$\n",
    "\n",
    "Now, we want to minimize this loss using backpropagation.\n",
    "\n",
    "### 1. Compute the error terms at the output layer:\n",
    "\n",
    "$ \\delta_{o1} = (o_1 - y_1) \\cdot \\sigma'(o_1)$\n",
    "\n",
    "$ \\delta_{o2} = (o_2 - y_2) \\cdot \\sigma'(o_2)$\n",
    "\n",
    "Here, $\\sigma'$ is the derivative of the activation function used in the output layer (for simplicity, assume a linear activation, so $\\sigma'(x) = 1$).\n",
    "\n",
    "### 2. Compute the error terms at the hidden layer:\n",
    "\n",
    "$ \\delta_{h1} = (\\delta_{o1} \\cdot w_{31} + \\delta_{o2} \\cdot w_{32}) \\cdot \\sigma'(h1)$\n",
    "\n",
    "$ \\delta_{h2} = (\\delta_{o1} \\cdot w_{41} + \\delta_{o2} \\cdot w_{42}) \\cdot \\sigma'(h2)$\n",
    "\n",
    "\n",
    "### 3. Update the weights and biases using the error terms and learning rate ($\\alpha$):\n",
    "\n",
    "$ w_{31}^{new} = w_{31} - \\alpha \\cdot \\delta_{o1} \\cdot h_1$\n",
    "\n",
    "$ w_{41}^{new} = w_{41} - \\alpha \\cdot \\delta_{o1} \\cdot h_2$\n",
    "\n",
    "$ w_{32}^{new} = w_{32} - \\alpha \\cdot \\delta_{o2} \\cdot h_1$\n",
    "\n",
    "$ w_{42}^{new} = w_{42} - \\alpha \\cdot \\delta_{o2} \\cdot h_2$\n",
    "\n",
    "$ w_{11}^{new} = w_{11} - \\alpha \\cdot \\delta_{h1} \\cdot x_1$\n",
    "\n",
    "$ w_{21}^{new} = w_{21} - \\alpha \\cdot \\delta_{h1} \\cdot x_2$\n",
    "\n",
    "$ w_{12}^{new} = w_{12} - \\alpha \\cdot \\delta_{h2} \\cdot x_1$\n",
    "\n",
    "$ w_{22}^{new} = w_{22} - \\alpha \\cdot \\delta_{h2} \\cdot x_2$\n",
    "\n",
    "\n",
    "### 4. Update the biases similarly:\n",
    "\n",
    "$ b_3^{new} = b_3 - \\alpha \\cdot \\delta_{o1}$\n",
    "\n",
    "$ b_4^{new} = b_4 - \\alpha \\cdot \\delta_{o2}$\n",
    "\n",
    "$ b_1^{new} = b_1 - \\alpha \\cdot \\delta_{h1}$\n",
    "\n",
    "$ b_2^{new} = b_2 - \\alpha \\cdot \\delta_{h2}$\n",
    "\n",
    "This process is repeated iteratively until the network converges to a solution that minimizes the error. The learning rate ($\\alpha$) is a hyperparameter that determines the step size during weight and bias updates. Adjustments may be needed based on the specific problem and dataset.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
