{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# unit 3.5 - Recurrent neural networks (RNN)\n",
    "\n",
    "Recurrent neural networks or RNN are used to learn data sequences. They are a neural architecture that has important history and that has been supplanted by Transformers since the year 2017. RNN can still have meaningful application in problems with relatively small context windows.\n",
    "\n",
    "Examples of data sequences:\n",
    "- letters in words\n",
    "- phonemes in speech\n",
    "- predict the next word, language modeling\n",
    "- stock market prediction\n",
    "- temperature in a location\n",
    "\n",
    "## Types of RNN\n",
    "\n",
    "There are several types of RNN, depending on the task at hand. \n",
    "\n",
    "- 1 to 1: predict a value (temperature) from a single previous value\n",
    "- 1 to N: generate a caption for an image\n",
    "- N to N: translate a language to another\n",
    "\n",
    "![rnn types](images/rnn1.png)\n",
    "\n",
    "## The basic RNN cell\n",
    "\n",
    "Remember that RNN are designed to learn a sequence. As such they need to take an input at each time step, but they also need to \"remember\" previous input using a \"state\" variable.\n",
    "\n",
    "The basic RNN cell is a simple neural network layer with 2 inputs instead of one. In most cases the two inputs are concatenated to form one input. The 2 inputs are:\n",
    "- the data input at time $t$\n",
    "- the state at $t-1$\n",
    "\n",
    "![rnn cells](images/rnn2.png)\n",
    "\n",
    "RNN cells basically process the two input multiple times with the same weights. They can unroll in time as a linear layer that takes 2 inputs. \n",
    "\n",
    "This is called \"vanilla\" RNN.\n",
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "The vanilla RNN network has trouble recalling long past information, as the inputs have to travel through small wights multiple times (see section below \"Limitation of RNN and why Transfomers\").\n",
    "\n",
    "As a results AI researchers developed the Long-Short Term Memory or LSTM cell. The LSTM cell has an input $x$, an output $h$ and propagates two signals from cell to cell.\n",
    "\n",
    "![LSTM1](images/lstm1.png)\n",
    "\n",
    "Note the symbols here are:\n",
    "\n",
    "![LSTM1s](images/lstm_symbols.png)\n",
    "\n",
    "One of the signal that is propagate from cell to cell is the \"Cell\" state signal $C$. This signal can be modulated by a weight and information can be added to it. These two modification are applied on the multiplication and addition symbols.\n",
    "\n",
    "![LSTM2](images/lstm2.png)\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Information from the input $x$ and previous state $h$ can be processed by a linearl layer called \"forget\" or $f$. A sigmoid non-linearity gives the \"forget\" signal the ability to swing between positive and negative numbers. This signal is what controls how much cell state $C$ is propagated to the next time-step.\n",
    "\n",
    "![LSTM3](images/lstm3.png)\n",
    "\n",
    "The input $x$ and previous hidden state $h$ are used by two more linear layer for processing. One is the \"input\" $i$ linear layer, and one if the \"new Cell state\".\n",
    "\n",
    "\n",
    "![LSTM4](images/lstm4.png)\n",
    "\n",
    "\n",
    "This is how the Cell state is modulated:\n",
    "\n",
    "![LSTM5](images/lstm5.png)\n",
    "\n",
    "The output $o$ and stage $h$ are modulated by another output linear layer:\n",
    "\n",
    "![LSTM6](images/lstm6.png)\n",
    "\n",
    "\n",
    "## GRU\n",
    "\n",
    "The Gated-Recurrent-Unit or GRU is a modification of the LSTM cell that uses 4 equations instead of 5 and forgoes one internal state. As such it is slightly more efficient than an LSTM. The improvement is minor, though, and can only be useful in severely resource constrained computational platforms, such as edge computing on micro-controllers.\n",
    "\n",
    "![GRU](images/gru1.png)\n",
    "\n",
    "Note: Some of this material derives from the [post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (with C. Colah permission).\n",
    "\n",
    "## Limitation of RNN and why Transfomers\n",
    "\n",
    "Limitation: \n",
    "- cannot remember easily more than ~100 steps in the past\n",
    "- limited by fixed weight\n",
    "- does not build a knowledge graph\n",
    "\n",
    "Vanishing gradient problem: weights (<1) are multiplied many times by other weights <1:\n",
    "As an example imagine the input goes through 3 weights of 0.1 value: the input after 3 time steps become 0.1 x 0.1 x 0.1 x 0.1 = 0.0001. The inputs are becoming smaller and smaller at each time step, and thus the RNN loses the ability to learn from the past.\n",
    "\n",
    "Transformer neural networks have proven to be able to create more complex input filters and vast knowledge graphs on inputs. As such they rendered RNN and LSTM virtually obsolete in 2017 with this [paper](https://arxiv.org/abs/1706.03762)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
