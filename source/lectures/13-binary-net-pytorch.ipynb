{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit 1.3 - PyTorch Training \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/culurciello/deep-learning-course-source/blob/main/source/lectures/13-AND-train-pytorch.ipynb)\n",
    "\n",
    "Here we will use Pytorch to train a neural network for the AND function\n",
    "\n",
    "Train = find the values of the weights automatically / no trial and error!\n",
    "\n",
    "For training, we will need a lot of examples. Examples are in the form of: {input, desired_output} \n",
    "\n",
    "desired_output is also called ground_truth or label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d415bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 2)\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        output = self.l2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This create a neural network in pytorch in the \"proper\" way. You can do this in many ways, but this is one way professionals in AI use!\n",
    "\n",
    "NOW!!!!\n",
    "\n",
    "To train we will use an algorihtm called \"gradient descent\"\n",
    "\n",
    "We define a \"train\" function that can:\n",
    "- take all the examples we have\n",
    "- run the network on inputs\n",
    "- compare the network output to the ground truth\n",
    "- compute a measure of error\n",
    "- back-propagate the error\n",
    "- adjust the weights\n",
    "- repeat for all samples in dataset\n",
    "\n",
    "network = neural network to train\n",
    "\n",
    "Optimizer = grandient descent algorithm to update the weights\n",
    "\n",
    "train _loader = loads examples from a dataset / database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5b4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4 (0%)]\tLoss: 0.000133\n",
      "Train Epoch: 2 [0/4 (0%)]\tLoss: 0.122131\n",
      "Train Epoch: 3 [0/4 (0%)]\tLoss: 0.151933\n",
      "Train Epoch: 4 [0/4 (0%)]\tLoss: 0.089941\n",
      "Train Epoch: 5 [0/4 (0%)]\tLoss: 0.023557\n",
      "Train Epoch: 6 [0/4 (0%)]\tLoss: 0.000799\n",
      "Train Epoch: 7 [0/4 (0%)]\tLoss: 0.002576\n",
      "Train Epoch: 8 [0/4 (0%)]\tLoss: 0.014207\n",
      "Train Epoch: 9 [0/4 (0%)]\tLoss: 0.038693\n",
      "Train Epoch: 10 [0/4 (0%)]\tLoss: 0.057657\n",
      "Train Epoch: 11 [0/4 (0%)]\tLoss: 0.048526\n",
      "Train Epoch: 12 [0/4 (0%)]\tLoss: 0.030564\n",
      "Train Epoch: 13 [0/4 (0%)]\tLoss: 0.015616\n",
      "Train Epoch: 14 [0/4 (0%)]\tLoss: 0.004006\n",
      "Train Epoch: 15 [0/4 (0%)]\tLoss: 0.000244\n",
      "Train Epoch: 16 [0/4 (0%)]\tLoss: 0.000034\n",
      "Train Epoch: 17 [0/4 (0%)]\tLoss: 0.000482\n",
      "Train Epoch: 18 [0/4 (0%)]\tLoss: 0.001684\n",
      "Train Epoch: 19 [0/4 (0%)]\tLoss: 0.002919\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = torch.Tensor(data)\n",
    "        target = torch.Tensor([target])\n",
    "#         print(batch_idx, data, target)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "#         loss = 1/2*(output-target).pow(2).sum()\n",
    "        loss = F.mse_loss(output, target)\n",
    "        \n",
    "        # backward pass\n",
    "#         for p in model.parameters():\n",
    "#             p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # weight update:\n",
    "#         lr = 0.1\n",
    "#         for p in model.parameters():\n",
    "#             p.data += -lr * p.grad\n",
    "\n",
    "        if batch_idx % 4 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "# dataset: logic AND!\n",
    "train_loader = [((0,0),0),((0,1),0),((1,0),0),((1,1),1)] # inputs, outputs examples (4)\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train(model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bdca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0571]]),\n",
       " tensor([[0.0054]]),\n",
       " tensor([[-0.0043]]),\n",
       " tensor([[1.0001]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model trained on logic AND:\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inp = torch.Tensor([(0,0)])\n",
    "    o1 = model(inp)\n",
    "    inp = torch.Tensor([(0,1)])\n",
    "    o2 = model(inp)\n",
    "    inp = torch.Tensor([(1,0)])\n",
    "    o3 = model(inp)\n",
    "    inp = torch.Tensor([(1,1)])\n",
    "    o4 = model(inp)\n",
    "    \n",
    "o1,o2,o3,o4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the interesting portions to dig deeper are:\n",
    "\n",
    "- what is the [loss](https://pytorch.org/docs/stable/nn.html#loss-functions)?\n",
    "- what is the [optimizer](https://pytorch.org/docs/stable/optim.html)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOW DO I LEARN MORE on gradient descent?\n",
    "\n",
    "To go forward, you have some options:\n",
    "\n",
    "1- learn to use pytorch optimization as is\n",
    "\n",
    "- for this you are done!\n",
    "\n",
    "2- learn more details about back-propagation but just enough\n",
    "\n",
    "- See [this step-by-step example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
    "\n",
    "3- learn how backpropagation in pytorch works\n",
    "\n",
    "- This [series is amazing](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
